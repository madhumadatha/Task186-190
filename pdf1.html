<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="pdf1.css">
    </head>
    <body>
        <p class="A">Kostek, B. (1998).  Computer-based recognition of musical phrases using the rough set
 approach. Journal of Information Science, 104, 15-30.
        </p>
        <p class="A"> Kryszkiewicz, M. (1994).  Knowledge reduction algorithms in information systems.  Ph.D.
 thesis, Faculty of Electronics, Warsaw University of Technology, Poland.
        </p>
        <p class="A"> Loupe, P. S., Freeman, R. L., Grzymala-Busse, J. W., & Schroeder, S. R. (2001).  Using rule
 induction for prediction of self-injuring behavior in animal models of development
 disabilities.  14th IEEE Symposium on Computer-Based Medical Systems, CBMS
 2001, July 26-27-, pp. 171–176, Bethesda, MD, IEEE Press.
        </p>
        <p class="A"> Maheswari, U., Siromoney, A., Mehata, K., & Inoue, K. (2001).  The variable precision
 rough set inductive logic programming model and strings. Computational Intel
ligence, 17, 460-471.
        </p>
        <p class="A"> Michalski, R. S. (1983).  A theory and methodology of inductive learning. In R.S.
 Michalski, J.G. Carbonell, & T.M. Mitchell (eds.): Machine Learning. An Artificial
 Intelligence Approach, pp.83-134. San Francisco, CA: Morgan Kauffman.
        </p>
        <p class="A"> Michalski, R.S., Carbonell J.G., & Mitchell, T.M. (1983). Machine learning. An artificial
 intelligence approach. San Francisco, CA: Morgan Kauffman.
        </p>
        <p class="A"> Michalski, R. S., Mozetic, I., Hong, J. & Lavrac, N. (1986). The AQ15 inductive learning
 system: An overview and experiments. Report UIUCDCD-R-86-1260, Department
 of Computer Science, University of Illinois.Moradi, H., Grzymala-Busse, J. W.,
 &Roberts, J. A. (1995).  Entropy of English text: Experiments with humans and a
 machine learning system based on rough sets.  2nd Annual Joint Conference on
 Information Sciences, JCIS’95, Wrightsville Beach, North Carolina, pp. 87–88, Paul
 R. Wang Press.
        </p>
        <p class="A">Mrozek, A. (1986).  Use of rough sets and decision tables for implementing rule-based
 control of industrial processes. Bulletin of the Polish Academy of Sciences, 34, 332
356.
        </p>
        <p class="A">Nguyen, H. S. (1998).  Discretization problems for rough set methods.  1st  International
 Conference on Rough Sets and Current Trends in Computing, Warsaw, Poland.
 Lecture Notes in AI 1424, pp.545-552. Berlin: Springer-Verlag.
        </p>
        <p class="A">Pawlak, Z. (1982).  Rough Sets.  International Journal of Computer and Information
 Sciences, 11, 341-356.
        </p>
        <p class="A">Pawlak, Z. (1984).  Rough Sets. International Journal of Man-Machine Studies 20, 469.
 Pawlak, Z. (1991).  Rough Sets.  Theoretical Aspects of Reasoning about Data.  Kluwer
 Academic Publishers, Dordrecht, Boston, London.
        </p>
        <p class="A">Pawlak, Z. (1996).  Rough sets, rough relations and rough functions. Fundamenta
 Informaticae, 27, 103-108.
        </p>
        <p class="A"> Pawlak, Z., Grzymala-Busse, J. W., Slowinski, R., & Ziarko, W. (1995).  Rough sets.
 Communications of the ACM 38, 89-95.
        </p>
        <p class="A"> Peters, J. Skowron, A. & Suraj, Z. (1999).  An application of rough set methods in control
 design. Workshop on Concurrency, Warsaw, pp. 214-235.
        </p>
        <p class="A"> Plonka, L.  & Mrozek, A. (1995).  Rule-based stabilization of the inverted pendulum.
 Computational Intelligence, 11, 348-356.
        </p>
        <p class="A">Polkowski, L. & Skowron, A. (1998).  Rough sets in knowledge discovery, 2, Applica
tions, case studies and software systems,  Appendix 2: Software Systems. pp. 551
601.   Heidelberg-New York: Physica Verlag.
</p>
        <p class="A"> Shafer, G.  (1976).  A mathematical theory of evidence. Princeton, NJ: Princeton
 University Press.
        </p><br><br>
        <p Class="A">Stefanowski, J. (1998).  On rough set based approaches to induction of decision rules.
 In L. Polkowski & A. Skowron (eds.), Rough sets in data mining and knowledge
 discovery, pp. 500-529. Heidelberg-New York: Physica-Verlag.
        </p>
        <p class="A">Woolery, L., Grzymala-Busse, J., Summers, S., & Budihardjo, A. (1991).  The use of
 machine learning program LERS_LB 2.5 in knowledge acquisition for expert system
 development in nursing.  Computers in Nursing 9, 227-234.
        </p>
        <p class="A"> Zhao, Z. (1993).  Rough set approach to speech recognition. M.Sc. thesis, Computer
 Science Department University of Regina, Canada.
        </p>
        <p class="A"> Ziarko, W. (1993).  Variable precision rough sets model. Journal of Computer and
 Systems Sciences, 46, 39-59.
        </p>
        <p class="A">Ziarko, W.  (1998a).  Approximation region-based decision tables.  International
 Conference on Rough Sets and Current Trends in Computing, Warsaw, Lecture
 Notes in AI  1424, pp.178-185.  Berlin: Springer-Verlag.
        </p>
        <p class="A">Ziarko, W.  (1998b).  KDD-R: Rough sets-based data mining system. In L. Polkowski &
 A. Skowron (eds.), Rough sets in knowledge discovery, Part II.  Studies in
 Fuzziness and  Soft Computing, pp. 598-601. Berlin: Springer-Verlag.
        </p>
        <p class="A"> Ziarko, W. (1999).  Decision making with probabilistic decision tables. 7th International.
 Workshop on Rough Sets, Fuzzy Sets, Data Mining and  Granular Computing,
 RSFDGrC’99, Yamaguchi, Japan, Lecture Notes in AI 1711, pp. 463-471. Berlin:
 Springer-Verlag.
        </p>
        <p class="A"> Ziarko, W. & Shan, N. (1996).  A method for computing all maximally general rules in
 attribute-value systems.  Computational Intelligence: an International Journal,
 12, 223-234.
        </p><br><br>
        <center><h1 style="font-size:25px;"class="A B"> Chapter VII</h1></center><br>
        <center><h2 style="font-size:60px;"class="A B">The Impact of Missing</h2></center>
        <center><h3 style="font-size:50px;"class="A B">Data on Data Mining</h3></center>
        <center><h4 style="font-size:15px;"class="A"> Marvin L. Brown</h4></center>
        <center><h5 style="font-size:15px;"class="A"> Hawaii Pacific University, USA</h5></center>
        <center><h6 style="font-size:15px;"class="A"> John F. Kros</h6></center>
        <center><h7 style="font-size:15px;"class="A"> East Carolina University, USA </h7></center><br>
        <center><h8 style="font-size:30px;"class="A B">  ABSTRACT </h8></center>
        <i><p class="A"> Data mining is based upon searching the concatenation of multiple databases that
 usually contain some amount of missing data along with a variable percentage of
 inaccurate data, pollution, outliers, and noise. The actual data-mining process deals
 significantly with prediction, estimation, classification, pattern recognition, and the
 development of association rules. Therefore, the significance of the analysis depends
 heavily on the accuracy of the database and on the chosen sample data to be used for
 model training and testing.  The issue of missing data must be addressed since ignoring
 this problem can introduce bias into the models being evaluated and lead to inaccurate
 data mining conclusions.
            </p></i><br>
        <center><h9 style="font-size:30px;"class="A B"> THE IMPACT OF MISSING DATA</h9></center>
        <p class="A">Missing or inconsistent data has been a pervasive problem in data analysis since
 the origin of data collection.  More historical data is being collected today due to the
 proliferation of computer software and the high capacity of storage media.  In turn, the
 issue of missing data becomes an even more pervasive dilemma.  An added complication
 is that the more data that is collected, the higher the likelihood of missing data. This will
 require one to address the problem of missing data in order to be effective.</p><br>
        <p class="A"> During the last four decades, statisticians have attempted to address the impact of
 missing data on information technology.</p>
        <p class="A">This chapter’s objectives are to address the impact of missing data and its impact
 on data mining.  The chapter commences with a background analysis, including a review
 of both seminal and current literature.  Reasons for data inconsistency along with
 definitions of various types of missing data are addressed.  The main thrust of the chapter
 focuses on methods of addressing missing data and the impact that missing data has on
 the knowledge discovery process.  Finally, trends regarding missing data and data mining
 are discussed in addition to future research opportunities and concluding remarks.</p>
      <h10 style="font-size:25px;"class="A B"> Background</h10> 
        <p class="A"> The analysis of missing data is a comparatively recent discipline.  With the advent
 of the mainframe computer in the 1960s, businesses were capable of collecting large
 amounts of data on their customer databases.  As large amounts of data were collected,
 the issue of missing data began to appear.  A number of works provide perspective on
 missing data and data mining.</p>
        <p class="A">Afifi and Elashoff (1966) provide a review of the literature regarding missing data
 and data mining.  Their paper contains many seminal concepts, however, the work may
 be dated for today’s use.  Hartley and Hocking (1971), in their paper entitled “The
 Analysis of Incomplete Data,” presented one of the first discussions on dealing with
 skewed and categorical data, especially maximum likelihood (ML) algorithms such as
 those used in Amos.  Orchard and Woodbury (1972) provide early reasoning for
 approaching missing data in data mining by using what is commonly referred to as an
 expectation maximization (EM) algorithm to produce unbiased estimates when the data
 are missing at random (MAR).  Dempster, Laird, and Rubin’s (1977) paper provided
 another method for obtaining ML estimates and using EM algorithms. The main
 difference between Dempster, Laird, and Rubin’s (1977) EM approach and that of Hartley
 and Hocking is the Full Information Maximum Likelihood (FIML) algorithm used by
 Amos.  In general, the FIML algorithm employs both first- and second-order derivatives
 whereas the EM algorithm uses only first-order derivatives.</p>
        <p class="A"> Little (1982) discussed models for nonresponse, while Little and Rubin (1987)
 considered statistical analysis with missing data.  Specifically, Little and Rubin (1987)
 defined three unique types of missing data mechanisms and provided parametric
 methods for handling these types of missing data.  These papers sparked numerous
 works in the area of missing data.  Diggle and Kenward (1994) addressed issues regarding
 data missing completely at random, data missing at random, and likelihood-based
 inference.  Graham, Hofer, Donaldson, MacKinnon, and Schafer (1997) discussed using
 the EM algorithm to estimate means and covariance matrices from incomplete data.
 Papers from Little (1995) and Little and Rubin (1989) extended the concept of ML
 estimation in data mining, but they also tended to concentrate on data that have a few
 distinct patterns of missing data.  Howell (1998) provided a good overview and examples
 of basic statistical calculations to handle missing data.</p>
        <p class="A"> The problem of missing data is a complex one.  Little and Rubin (1987) and Schafer
 (1997) provided conventional statistical methods for analyzing missing data and dis
cussed the negative implications of naïve imputation methods.  However, the statistical</p><br>
        <p class="A">literature on missing data deals almost exclusively with the training of models rather than
 prediction (Little, 1992).  Training is described as follows: when dealing with a small
 proportion of cases containing missing data, you can simply eliminate the missing cases
 for purposes of training.  Cases cannot be eliminated if any portion of the case is needed
 in any segment of the overall discovery process.</p>
        <p class="A">In theory, Bayesian methods can be used to ameliorate this issue.  However,
 Bayesian methods have strong assumptions associated with them.  Imputation methods
 are valuable alternatives to introduce here, as they can be interpreted as an approximate
 Bayesian inference for quantities of interest based on observed data.</p>
        <p class="A"> A number of articles have been published since the early 1990s regarding imputa
tion methodology. Schafer and Olsen (1998) and Schafer (1999) provided an excellent
 starting point for multiple imputation.  Rubin (1996) provided a detailed discussion on
 the interrelationship between the model used for imputation and the model used for
 analysis.  Schafer’s (1997) text has been considered a follow up to Rubin’s 1987 text.  A
 number of conceptual issues associated with imputation methods are clarified in Little
 (1992).  In addition, a number of case studies have been published regarding the use of
 imputation in medicine (Barnard & Meng, 1999; van Buuren, Boshuizen, & Knook, 1999)
 and in survey research (Clogg, Rubin, Schenker, Schultz, & Weidman, 1991). A number
 of researchers have begun to discuss specific imputation methods.  Hot deck imputation
 and nearest neighbor methods are very popular in practice, despite receiving little overall
 coverage with regard to Data Mining (see Ernst, 1980; Kalton & Kish, 1981; Ford, 1983;
 and David, Little, Samuhel, & Triest, 1986).</p>
        <p class="A"> Breiman, Friedman, Olshen, and Stone (1984) developed a method known as CART®,
 or classification and regression trees.  Classification trees are used to predict membership
 of cases or objects in the classes of categorical dependent variables from their measure
ments on one or more predictor variables.  Loh and Shih (1997) expanded on classification
 trees with their paper regarding split selection methods.  Some popular classification tree
 programs include FACT (Loh & Vanichestakul, 1988), THAID (Morgan & Messenger,
 1973), as well as the related programs AID, for Automatic Interaction Detection (Morgan
 & Sonquist, 1963), and CHAID, for Chi-Square Automatic Interaction Detection (Kass,
 1980).  Classification trees are useful data- mining techniques as they are easily
 understood by business practitioners and are easy to perceive visually. Also, the basic
 tree induction algorithm is considered to be a “greedy” approach to classification, using
 a recursive divide-and-conquer approach (Han & Kamber, 2001). This allows for raw data
 to be analyzed quickly without a great deal of preprocessing. No data is lost, and outliers
 can be identified and dealt with immediately (Berson, Smith and Thearling, 2000).</p>
        <p class="A"> Agrawal, Imielinski, and Swami (1993) introduced association rules for the first time
 in their paper, “Mining Association Rules between Sets of Items in Large.” A second
 paper by Agrawal and Srikant (1994) introduced the Apriori algorithm.  This is the
 reference algorithm for the problem of finding Association Rules in a database.  Valuable
 “general purpose” chapters regarding the discovery of Association Rules are included
 in the texts: Fast Discovery of Association Rules by R. Agrawal, H. Mannila, R. Srikant,
 H. Toivonen and A. I. Verkamo, and Advances in Knowledge Discovery and Data
 Mining. Association rules in data-mining applications search for interesting relation
ships among a given set of attributes. Rule generation is based on the “interestingness”
 of a proposed rule, measured by satisfaction of thresholds for minimum support and</p>
    </body>
</html>